---
title: "AB testing data"
author: "Kevin Wang"
date: "`r paste0('Initiated on 2018 Apr 02, compiled on ', format(Sys.time(), '%Y %b %d'))`"
output:
  html_document:
    code_folding: hide
    fig_height: 12
    fig_width: 12
    toc: yes
    number_sections: true
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

# Loading packages
```{r}
library(tidyverse)
library(readxl)
library(skimr)
library(janitor)
```

# Loading functions
```{r}
theme_set(theme_classic(18) +
            theme(legend.position = "bottom"))
```


# Loading data
```{r}
raw_data = read_csv("ab_data.csv") %>% 
  dplyr::mutate(user_id = user_id %>% as.character, 
                converted = converted %>% as.character) %>% 
  dplyr::arrange(timestamp) %>% 
  group_by(user_id) %>% 
  add_count(name = "user_count") %>% 
  ungroup()
```


# Basic reporting of characteristics

```{r}
raw_data$user_count %>% table
```

Note that there are multiple sessions for a single given user. 

The landing page and the group seem to be duplicated information. Let's check.

```{r}
raw_data %>% 
  tabyl(group, landing_page) %>%
  janitor::adorn_totals()

raw_data %>% 
  tabyl(group, landing_page) %>%
  janitor::adorn_totals(where = "col")

raw_data %>% 
  tabyl(group, landing_page) %>%
  janitor::adorn_percentages(denominator = "all")
```

No, it is not, but the original data descriptions did not say what are the differences between the two in the original design of the experiment. 

Note that the totals for the samples are too perfect. So I wouldn't be surprised if the data has ben manually cut into equal proportions. 


## Using only aligned data

```{r}
aligned_data = raw_data %>% 
  dplyr::filter((landing_page == "new_page" & group == "treatment") | 
                  (landing_page == "old_page" & group == "control"))

dim(aligned_data)

aligned_data %>% 
  tabyl(group, landing_page) %>%
  janitor::adorn_percentages(denominator = "all")
```


## Getting duplicated users from 
```{r}
aligned_data %>% 
  janitor::get_dupes(user_id)
```


So, based on this output, I would guess all the mis-aligned entries are those users who visited the site more than once and landed on both sides of control and treatment. Let's check

```{r}
raw_data %>% 
  tabyl(group, landing_page, user_count) %>%
  janitor::adorn_totals()
```


Yes, we see that if the user only visited once, then these entries are always aligned. But if users visited the site more than once, then that is a bit different. 

It will be interesting to know how differently the returning users behave compare to the unique visited users. So this is our hypothesis based on the IDE: **returning users will have a higher conversion rate than the unique visited users**. Let's try some visualisations first. 


# Visualisations
```{r}
summ_data = raw_data %>% 
  group_by(group, landing_page, user_count, converted) %>% 
  tally() %>% 
  dplyr::mutate(perc_converted = n/sum(n))

summ_data

summ_data %>% 
  ggplot(aes(x = factor(user_count), y = n, 
             fill = converted)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = signif(perc_converted, 2)),
            position = position_dodge(width = 0.9)) +
  facet_grid(group ~ landing_page, scales = "free") +
  scale_y_continuous(labels = scales::comma)
```


Based on this plot, I am pretty confident in saying that there won't be a significant difference between the two versions, even tyring to resolve the data at the level of multiple users. So let's proceed to performing the proportion test, for multiple users and for single users. 


# Single users
```{r}
single_data = raw_data %>% 
  dplyr::filter(user_count == 1)

single_table = single_data %>% 
  tabyl(landing_page, converted) %>%
  janitor::adorn_totals(where = "col")

single_table

prop.test(x = single_table[,"1"],
          n = single_table[,"Total"])
```


# Multiple users
```{r}
multiple_data = raw_data %>% 
  dplyr::filter(user_count == 2)

multiple_table = multiple_data %>% 
  tabyl(landing_page, converted) %>%
  janitor::adorn_totals(where = "col")

multiple_table

prop.test(x = single_table[,"1"],
          n = single_table[,"Total"])
```


## Logistic regression 

It is still unclear to me why would you be placed in a control group and end up with the new_page, or in the treatment group and end up with the old_page. But if these two variables are genuinely different, then we should do some investigations using logistic regression to check up on the proportions. 

```{r}
glm(as.integer(converted) ~ group * landing_page, 
    family = "binomial", data = multiple_data) %>% 
  broom::tidy()
```

# Are latter visits by the users more successful?

If a user visited the site multiple times, can we check if the latter visit is more successful at conversion?

```{r}
multiple_data = multiple_data %>% 
  group_by(user_id) %>% 
  dplyr::mutate(index_visit = seq_len(n())) %>% 
  ungroup()

multiple_data %>% 
   tabyl(index_visit, converted, landing_page) %>%
  # janitor::adorn_totals(where = "col") %>% 
  janitor::adorn_percentages(denominator = "row")
```


# Sample size calculations

If the original experiment is aimed at testing for only 1-2% effects, then it will be interesting to know if the number of samples we obtained are sufficient. We will operate on the single users only. 

```{r}
single_data %>% dim

library(pwr)

pwr::pwr.p.test(h = 0.01, n = NULL,
                sig.level = 0.05, power = 0.8, 
                alternative = "two.sided")

pwr::pwr.p.test(h = 0.02, n = NULL,
                sig.level = 0.05, power = 0.8, 
                alternative = "two.sided")

pwr.p.test(h = ES.h(p1 = 0.13, p2 = 0.12),
           sig.level = 0.05,
           power = 0.80,
           alternative = "two.sided")

pwr.2p.test(h = ES.h(p1 = 0.13, p2 = 0.12),
           sig.level = 0.05,
           power = 0.80,
           alternative = "two.sided")
```


# Session Info
```{r}
sessioninfo::session_info()
```

